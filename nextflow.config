// DIANN Workflow - Base Configuration
// Compatible with Nextflow DSL2

// Enable DSL2
nextflow.enable.dsl = 2

// Default parameters - can be overridden via config files or command line
params {
    // Container settings
    container_registry = 'quay.io/karlssoc/diann'
    diann_version = '2.3.1'

    // Default resource settings
    threads = 60

    // Parallel execution mode (for 60-core systems)
    // When true: runs 2 jobs at 30 cores each for better throughput
    // When false: runs 1 job at 60 cores for maximum per-job speed
    parallel_mode = true

    // Output directory
    outdir = 'results'

    // SLURM settings (can be overridden)
    slurm_account = null
    slurm_queue = null      // IMPORTANT: Set this if your SLURM requires a partition (-p flag)
                            // Example: slurm_queue = 'long' for long-running jobs
                            // Without this, jobs may fail or use restrictive default partition
    slurm_nodelist = null   // Optional: Specific node(s) to use (--nodelist flag)
                            // Example: slurm_nodelist = 'alap759' to use only that node
                            // WARNING: Using a specific node reduces parallelism

    // Optional model files for tuned workflows
    tokens = null
    rt_model = null
    im_model = null
    fr_model = null

    // Optional workflow parameters
    library = null
    subdir = null
    ref_library = null          // Reference library for batch calibration

    // Dynamic time allocation for quantify jobs
    time_base_hours = 2         // Base time in hours
    time_per_file_minutes = 10  // Additional time per MS file (accounts for variability)

    // Ultrafast quantification mode (trades some sensitivity for speed)
    ultrafast = false

    // Batch correction parameters (for multi-batch data from same instrument)
    individual_windows = false  // Use individual RT windows per run

    // Help message
    help = false
}

// Container configuration (enabled per-profile below)
// singularity {
//     enabled = true
//     autoMounts = true
//     cacheDir = "${HOME}/.singularity/cache"
// }

docker {
    enabled = false
}

// Process defaults
process {
    // Default container for all processes
    container = "${params.container_registry}:${params.diann_version}"

    // Error strategy
    errorStrategy = 'retry'
    maxRetries = 2

    // Default resources
    cpus = 1
    memory = '4 GB'
    time = '1h'
}

// Execution profiles
profiles {
    standard {
        process.executor = 'local'
        singularity {
            enabled = true
            autoMounts = true
            cacheDir = "${HOME}/.singularity/cache"
        }

        process {
            // Process-specific resources for local execution
            withLabel: 'diann_tune' {
                cpus = 10
                memory = '10 GB'
                time = '2h'
            }

            withLabel: 'diann_library' {
                cpus = params.threads
                memory = '30 GB'
                time = '4h'
            }

            withLabel: 'diann_quantify' {
                cpus = params.threads
                memory = { 500.MB * params.threads }
                time = '8h'
            }
        }
    }

    slurm {
        singularity {
            enabled = true
            autoMounts = true
            cacheDir = "${HOME}/.singularity/cache"
        }

        executor {
            name = 'slurm'
            queueSize = params.parallel_mode ? 4 : 2  // More concurrent jobs in parallel mode
            exitReadTimeout = '4 min'  // SLURM MinJobAge is 5min, must be less than that
            pollInterval = '15 sec'  // Check job status frequently
            queueStatInterval = '1 min'  // Check queue state frequently
            submitRateLimit = '10 / 1 sec'  // Throttle job submissions
        }

        process {
            executor = 'slurm'
            queue = params.slurm_queue
            clusterOptions = {
                def opts = []
                if (params.slurm_account) opts << "--account=${params.slurm_account}"
                if (params.slurm_nodelist) opts << "--nodelist=${params.slurm_nodelist}"
                opts.join(' ')
            }

            // Process-specific resources
            withLabel: 'diann_tune' {
                cpus = 10  // Tuning is not CPU-intensive, keep at 10
                memory = '10 GB'
                time = '2h'
            }

            withLabel: 'diann_library' {
                // Parallel mode: 30 cores allows 2 jobs concurrently (60 total)
                // Sequential mode: 60 cores for maximum speed per job
                cpus = params.parallel_mode ? 30 : params.threads
                memory = params.parallel_mode ? '20 GB' : '30 GB'
                time = '4h'
                maxForks = params.parallel_mode ? 2 : 1
            }

            withLabel: 'diann_quantify' {
                // Parallel mode: 30 cores allows 2 samples concurrently (60 total)
                // Sequential mode: 60 cores for maximum speed per sample
                cpus = params.parallel_mode ? 30 : params.threads
                memory = params.parallel_mode ? 15.GB : (500.MB * params.threads)
                // Note: time is dynamically calculated per job based on file count
                // Formula: time_base_hours + (file_count * time_per_file_minutes)
                // This fallback is only used if dynamic calculation fails
                time = '8h'
                maxForks = params.parallel_mode ? 2 : 1
            }
        }
    }

    docker {
        process.executor = 'local'
        docker {
            enabled = true
            runOptions = '-u $(id -u):$(id -g) --platform linux/amd64'
        }
    }

    docker_slurm {
        docker {
            enabled = true
            runOptions = '-u $(id -u):$(id -g) --platform linux/amd64'
        }

        executor {
            name = 'slurm'
            queueSize = params.parallel_mode ? 4 : 2  // More concurrent jobs in parallel mode
            exitReadTimeout = '4 min'  // SLURM MinJobAge is 5min, must be less than that
            pollInterval = '15 sec'  // Check job status frequently
            queueStatInterval = '1 min'  // Check queue state frequently
            submitRateLimit = '10 / 1 sec'  // Throttle job submissions
        }

        process {
            executor = 'slurm'
            queue = params.slurm_queue
            clusterOptions = {
                def opts = []
                if (params.slurm_account) opts << "--account=${params.slurm_account}"
                if (params.slurm_nodelist) opts << "--nodelist=${params.slurm_nodelist}"
                opts.join(' ')
            }

            // Process-specific resources
            withLabel: 'diann_tune' {
                cpus = 10  // Tuning is not CPU-intensive, keep at 10
                memory = '10 GB'
                time = '2h'
            }

            withLabel: 'diann_library' {
                // Parallel mode: 30 cores allows 2 jobs concurrently (60 total)
                // Sequential mode: 60 cores for maximum speed per job
                cpus = params.parallel_mode ? 30 : params.threads
                memory = params.parallel_mode ? '20 GB' : '30 GB'
                time = '4h'
                maxForks = params.parallel_mode ? 2 : 1
            }

            withLabel: 'diann_quantify' {
                // Parallel mode: 30 cores allows 2 samples concurrently (60 total)
                // Sequential mode: 60 cores for maximum speed per sample
                cpus = params.parallel_mode ? 30 : params.threads
                memory = params.parallel_mode ? 15.GB : (500.MB * params.threads)
                // Note: time is dynamically calculated per job based on file count
                // Formula: time_base_hours + (file_count * time_per_file_minutes)
                // This fallback is only used if dynamic calculation fails
                time = '8h'
                maxForks = params.parallel_mode ? 2 : 1
            }
        }
    }

    podman {
        process.executor = 'local'
        podman {
            enabled = true
        }
    }

    podman_slurm {
        podman {
            enabled = true
        }

        executor {
            name = 'slurm'
            queueSize = params.parallel_mode ? 4 : 2  // More concurrent jobs in parallel mode
            exitReadTimeout = '4 min'  // SLURM MinJobAge is 5min, must be less than that
            pollInterval = '15 sec'  // Check job status frequently
            queueStatInterval = '1 min'  // Check queue state frequently
            submitRateLimit = '10 / 1 sec'  // Throttle job submissions
        }

        process {
            executor = 'slurm'
            queue = params.slurm_queue
            clusterOptions = {
                def opts = []
                if (params.slurm_account) opts << "--account=${params.slurm_account}"
                if (params.slurm_nodelist) opts << "--nodelist=${params.slurm_nodelist}"
                opts.join(' ')
            }

            // Process-specific resources
            withLabel: 'diann_tune' {
                cpus = 10  // Tuning is not CPU-intensive, keep at 10
                memory = '10 GB'
                time = '2h'
            }

            withLabel: 'diann_library' {
                // Parallel mode: 30 cores allows 2 jobs concurrently (60 total)
                // Sequential mode: 60 cores for maximum speed per job
                cpus = params.parallel_mode ? 30 : params.threads
                memory = params.parallel_mode ? '20 GB' : '30 GB'
                time = '4h'
                maxForks = params.parallel_mode ? 2 : 1
            }

            withLabel: 'diann_quantify' {
                // Parallel mode: 30 cores allows 2 samples concurrently (60 total)
                // Sequential mode: 60 cores for maximum speed per sample
                cpus = params.parallel_mode ? 30 : params.threads
                memory = params.parallel_mode ? 15.GB : (500.MB * params.threads)
                // Note: time is dynamically calculated per job based on file count
                // Formula: time_base_hours + (file_count * time_per_file_minutes)
                // This fallback is only used if dynamic calculation fails
                time = '8h'
                maxForks = params.parallel_mode ? 2 : 1
            }
        }
    }

    cosmos {
        // COSMOS HPC cluster at LUNARC (https://www.lunarc.lu.se/systems/cosmos)
        // Optimized for AMD Milan nodes: 48 cores, 256 GB RAM, 2 TB local disk

        singularity {
            enabled = true
            autoMounts = true
            cacheDir = "${HOME}/.singularity/cache"
        }

        executor {
            name = 'slurm'
            queueSize = params.parallel_mode ? 4 : 2
            exitReadTimeout = '4 min'
            pollInterval = '15 sec'
            queueStatInterval = '1 min'
            submitRateLimit = '10 / 1 sec'
        }

        process {
            executor = 'slurm'
            queue = params.slurm_queue ?: 'lu'  // Default to 'lu' partition if not specified
            clusterOptions = {
                def opts = []
                if (params.slurm_account) opts << "--account=${params.slurm_account}"
                // Note: slurm_nodelist intentionally not used on shared HPC cluster
                opts.join(' ')
            }

            // Process-specific resources optimized for COSMOS
            withLabel: 'diann_tune' {
                cpus = 10
                memory = '10 GB'
                time = '2h'
                // Use local disk for better I/O performance
                scratch = '$SNIC_TMP'
            }

            withLabel: 'diann_library' {
                // Parallel mode: 24 cores allows 2 jobs concurrently (48 total)
                // Sequential mode: 48 cores for maximum speed per job
                cpus = params.parallel_mode ? 24 : (params.threads <= 48 ? params.threads : 48)
                memory = params.parallel_mode ? '20 GB' : '30 GB'
                time = '4h'
                maxForks = params.parallel_mode ? 2 : 1
                // Library generation is less I/O intensive, scratch optional
            }

            withLabel: 'diann_quantify' {
                // Parallel mode: 24 cores allows 2 samples concurrently (48 total)
                // Sequential mode: 48 cores for maximum speed per sample
                cpus = params.parallel_mode ? 24 : (params.threads <= 48 ? params.threads : 48)
                memory = params.parallel_mode ? 15.GB : (500.MB * (params.threads <= 48 ? params.threads : 48))
                time = '8h'
                maxForks = params.parallel_mode ? 2 : 1
                // CRITICAL: Use local disk for MS file I/O (10-50x faster)
                // COSMOS nodes have 2 TB local disk at $SNIC_TMP
                // Nextflow automatically stages inputs/outputs to/from local disk
                scratch = '$SNIC_TMP'
            }
        }

        // Override default threads for COSMOS (AMD Milan = 48 cores)
        params {
            threads = 48
        }
    }

    test {
        process {
            executor = 'local'
            cpus = 2
            memory = '4 GB'
        }
        params {
            threads = 2
        }
    }
}

// Execution report settings
timeline {
    enabled = true
    file = "${params.outdir}/pipeline_info/execution_timeline.html"
}

report {
    enabled = true
    file = "${params.outdir}/pipeline_info/execution_report.html"
}

trace {
    enabled = true
    file = "${params.outdir}/pipeline_info/execution_trace.txt"
}

dag {
    enabled = true
    file = "${params.outdir}/pipeline_info/pipeline_dag.svg"
}

// Manifest
manifest {
    name = 'diann-workflow'
    author = 'karlssoc'
    description = 'Modular Nextflow workflow for DIA-NN mass spectrometry analysis'
    mainScript = 'main.nf'
    nextflowVersion = '>=21.04.0'
    version = '1.0.0'
}
